{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOx3AnMRHeNT/X0k491pDF3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rts1988/Duolingo_spaced_repetition/blob/main/2A_WordVectors_Duolingo_spaced_repetition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'cornflowerblue' size=4>Getting multilingual word vectors and partially pre-processing them</font>"
      ],
      "metadata": {
        "id": "d6JkLmRvpVGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bz2\n",
        "import pickle\n",
        "import _pickle as cPickle\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def decompress_pickle(file):\n",
        " data = bz2.BZ2File(file, 'rb')\n",
        " data = cPickle.load(data)\n",
        " return data\n",
        "\n",
        "def compressed_pickle(title, data):  # do not add extension in filename\n",
        " with bz2.BZ2File(title + '.pbz2', 'w') as f: \n",
        "  cPickle.dump(data, f)\n",
        "\n",
        "path_name = '/content/drive/MyDrive/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDAm-LlIh1CA",
        "outputId": "77b1500a-fca6-45e8-ee8e-52f3f00b8b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='cornflowerblue' size= 3>Getting multilingual word vectors</font>"
      ],
      "metadata": {
        "id": "tDuLm_xCdQiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multilingual word vectors:\n",
        "https://www.cs.cmu.edu/~afm/projects/multilingual_embeddings.html\n",
        "\n",
        "did not use fasttext because it was taking too long to load (~15 min each language) and simply too big (4 GB each language)\n",
        "https://github.com/babylonhealth/fastText_multilingual"
      ],
      "metadata": {
        "id": "3sAtWIgTcRyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf /content/drive/MyDrive/multilingual_embeddings.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3qH81lpfnrT",
        "outputId": "14b16400-d219-415b-8bd4-086b173b1765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multilingual_embeddings.ar\n",
            "multilingual_embeddings.de\n",
            "multilingual_embeddings.en\n",
            "multilingual_embeddings.es\n",
            "multilingual_embeddings.fr\n",
            "multilingual_embeddings.it\n",
            "multilingual_embeddings.nl\n",
            "multilingual_embeddings.pb\n",
            "multilingual_embeddings.pl\n",
            "multilingual_embeddings.ro\n",
            "multilingual_embeddings.ru\n",
            "multilingual_embeddings.tr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arabic - ar\n",
        "Brazilian Portuguese - pb\n",
        "Dutch - nl (Netherlands?)\n",
        "English - en\n",
        "French - fr\n",
        "German - de\n",
        "Italian - it\n",
        "Polish - pl?\n",
        "Romanian - ro\n",
        "Russian - ru\n",
        "Spanish - es\n",
        "Turkish - tr\n"
      ],
      "metadata": {
        "id": "k2WpJc-eiU6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueAmkwNoiH83",
        "outputId": "3574069b-ba28-4e6a-b46f-76115480cad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive\t\t\t    multilingual_embeddings.nl\n",
            "multilingual_embeddings.ar  multilingual_embeddings.pb\n",
            "multilingual_embeddings.de  multilingual_embeddings.pl\n",
            "multilingual_embeddings.en  multilingual_embeddings.ro\n",
            "multilingual_embeddings.es  multilingual_embeddings.ru\n",
            "multilingual_embeddings.fr  multilingual_embeddings.tr\n",
            "multilingual_embeddings.it  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "german_embeddings = dict()\n",
        "count = 0\n",
        "with open('multilingual_embeddings.es','r') as f1:\n",
        "  lines = f1.readlines()\n",
        "for line in lines:\n",
        "  splitline = re.split('\\s+|\\n',line)\n",
        "  word = splitline[0]\n",
        "  emb  = splitline[1:]\n",
        "  german_embeddings[word] = emb\n",
        "  \n",
        "\n",
        "  #word = line[0]\n",
        "  #embedding = line[1:]\n",
        "  #german_embeddings[word] = embedding\n"
      ],
      "metadata": {
        "id": "-RHZnXDMhCpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def get_embeddings_dict(filename):\n",
        "  embeddings = dict()\n",
        "  count = 0\n",
        "  with open(filename,'r') as f1:\n",
        "    lines = f1.readlines()\n",
        "  for line in lines:\n",
        "    splitline = re.split('\\s+|\\n',line)\n",
        "    word = splitline[0]\n",
        "    emb_raw  = splitline[1:] \n",
        "    emb = np.array([float(n) for n in splitline[1:-1]]) # remove last element, and convert rest to float. \n",
        "    embeddings[word] = emb\n",
        "\n",
        "  return embeddings\n",
        "  \n",
        "  # currently embeddings is a list of string characterrs, the last element is an empty string.\n",
        "  # each value needs to be converted to a float value, and the last element should be removed. \n"
      ],
      "metadata": {
        "id": "nwGQ2yrKjhU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "german_embeddings = get_embeddings_dict('multilingual_embeddings.de')\n",
        "compressed_pickle(path_name+\"german_embeddings\",german_embeddings)\n",
        "french_embeddings = get_embeddings_dict('multilingual_embeddings.fr')\n",
        "compressed_pickle(path_name+\"french_embeddings\",french_embeddings)\n",
        "portuguese_embeddings = get_embeddings_dict('multilingual_embeddings.pb')\n",
        "compressed_pickle(path_name+\"portuguese_embeddings\",portuguese_embeddings)\n",
        "italian_embeddings = get_embeddings_dict('multilingual_embeddings.it')\n",
        "compressed_pickle(path_name+\"italian_embeddings\",italian_embeddings)\n",
        "english_embeddings = get_embeddings_dict('multilingual_embeddings.en')\n",
        "compressed_pickle(path_name+\"english_embeddings\",english_embeddings)\n",
        "spanish_embeddings = get_embeddings_dict('multilingual_embeddings.es')\n",
        "compressed_pickle(path_name+\"spanish_embeddings\",spanish_embeddings)"
      ],
      "metadata": {
        "id": "deUeUecRlgYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(german_embeddings.keys())[0:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrSYrk4IlAQF",
        "outputId": "068fca53-bd26-40f6-8bf0-20b36968c4c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['kürzerer',\n",
              " 'abzuschalten',\n",
              " 'beteiligen',\n",
              " 'fundraising',\n",
              " 'verwirrter',\n",
              " 'markenrechtsverletzungen',\n",
              " 'fachblatt',\n",
              " 'versorgen',\n",
              " 'familienzeit',\n",
              " 'sapiens',\n",
              " 'colaflasche',\n",
              " 'bulgaren',\n",
              " 'schneit',\n",
              " 'vorbeigekommen',\n",
              " 'spricht',\n",
              " 'schlagzeuger',\n",
              " 'männerbereiche',\n",
              " 'ballonfahrer',\n",
              " '146',\n",
              " 'kalash']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "german_embeddings['frau'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GHIJom6lANK",
        "outputId": "d220266a-ab48-4752-d193-d2f174a4a350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P8kVPgyVlAKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'cornflowerblue' size=3>Combining with lexeme_id</font>\n",
        "\n",
        "The word vectors for all languages have been saved to files above. \n",
        "\n",
        "They are combined with the lexeme_id below by the following steps:\n",
        "\n",
        "1. Create a separate dataframe lexeme_vec with index as surface_form, and columns lexeme_id, and lemma form and learning language. \n",
        "2. For each language, map the surface form column of the lexeme_vec dataframe with the respective word embeddings dictionary in a separate dataframe. \n",
        "2. Check how many words have vectors\n",
        "3. Deal with missing values by either mapping with lemma form, or other forms of imputation. \n"
      ],
      "metadata": {
        "id": "O6G3HtwWSHFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading all_lexemes from google drive:"
      ],
      "metadata": {
        "id": "zMuq7zbcTWEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "german_embeddings = decompress_pickle(path_name+\"german_embeddings.pbz2\")\n",
        "french_embeddings = decompress_pickle(path_name+\"french_embeddings.pbz2\")\n",
        "italian_embeddings = decompress_pickle(path_name+\"italian_embeddings.pbz2\")\n",
        "english_embeddings = decompress_pickle(path_name+\"english_embeddings.pbz2\")\n",
        "spanish_embeddings = decompress_pickle(path_name+\"spanish_embeddings.pbz2\")"
      ],
      "metadata": {
        "id": "BEdB32QsmMd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portuguese_embeddings = decompress_pickle(path_name+\"portuguese_embeddings.pbz2\")"
      ],
      "metadata": {
        "id": "gdZDj0K2nbMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes = decompress_pickle(path_name+\"Duolingo_all_lexemes.pbz2\")"
      ],
      "metadata": {
        "id": "cVqb126gTYq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "IHZQqeWaTTgB",
        "outputId": "ba337b82-f981-4095-d684-b9cda73432d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          lexeme_id learning_language  \\\n",
              "0  76390c1350a8dac31186187e2fe1e178                de   \n",
              "1  7dfd7086f3671685e2cf1c1da72796d7                de   \n",
              "2  35a54c25a2cda8127343f6a82e6f6b7d                de   \n",
              "3  0cf63ffe3dda158bc3dbd55682b355ae                de   \n",
              "4  84920990d78044db53c1b012f5bf9ab5                de   \n",
              "\n",
              "                      lexeme_string surface_form lemma_form    pos  \\\n",
              "0  lernt/lernen<vblex><pri><p3><sg>        lernt     lernen  vblex   \n",
              "1     die/die<det><def><f><sg><nom>          die        die    det   \n",
              "2          mann/mann<n><m><sg><nom>         mann       mann      n   \n",
              "3          frau/frau<n><f><sg><nom>         frau       frau      n   \n",
              "4    das/das<det><def><nt><sg><nom>          das        das    det   \n",
              "\n",
              "           modstrings  sf_length sf_translation lf_translation  \\\n",
              "0       [pri, p3, sg]          5         learns       to learn   \n",
              "1   [def, f, sg, nom]          3            the            the   \n",
              "2        [m, sg, nom]          4        husband        husband   \n",
              "3        [f, sg, nom]          4            Mrs            Mrs   \n",
              "4  [def, nt, sg, nom]          3            the            the   \n",
              "\n",
              "  surface_form_no_accents lemma_form_no_accents L_dist_word_tup_sf_noaccents  \\\n",
              "0                   lernt                lernen                  (2, learns)   \n",
              "1                     die                   die                     (2, the)   \n",
              "2                    mann                  mann                 (5, husband)   \n",
              "3                    frau                  frau                     (3, mrs)   \n",
              "4                     das                   das                     (3, the)   \n",
              "\n",
              "   L_dist_sf_noaccents  L_dist_sf_noaccents_norm  IDFword  EnglishIDF  \n",
              "0                    2                  0.400000   learns    6.981924  \n",
              "1                    2                  0.666667      the    0.001070  \n",
              "2                    5                  1.250000  husband    1.258263  \n",
              "3                    3                  0.750000      Mrs    6.309499  \n",
              "4                    3                  1.000000      the    0.001070  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48bba5b6-5167-46d1-9940-602addadd890\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lexeme_id</th>\n",
              "      <th>learning_language</th>\n",
              "      <th>lexeme_string</th>\n",
              "      <th>surface_form</th>\n",
              "      <th>lemma_form</th>\n",
              "      <th>pos</th>\n",
              "      <th>modstrings</th>\n",
              "      <th>sf_length</th>\n",
              "      <th>sf_translation</th>\n",
              "      <th>lf_translation</th>\n",
              "      <th>surface_form_no_accents</th>\n",
              "      <th>lemma_form_no_accents</th>\n",
              "      <th>L_dist_word_tup_sf_noaccents</th>\n",
              "      <th>L_dist_sf_noaccents</th>\n",
              "      <th>L_dist_sf_noaccents_norm</th>\n",
              "      <th>IDFword</th>\n",
              "      <th>EnglishIDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>76390c1350a8dac31186187e2fe1e178</td>\n",
              "      <td>de</td>\n",
              "      <td>lernt/lernen&lt;vblex&gt;&lt;pri&gt;&lt;p3&gt;&lt;sg&gt;</td>\n",
              "      <td>lernt</td>\n",
              "      <td>lernen</td>\n",
              "      <td>vblex</td>\n",
              "      <td>[pri, p3, sg]</td>\n",
              "      <td>5</td>\n",
              "      <td>learns</td>\n",
              "      <td>to learn</td>\n",
              "      <td>lernt</td>\n",
              "      <td>lernen</td>\n",
              "      <td>(2, learns)</td>\n",
              "      <td>2</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>learns</td>\n",
              "      <td>6.981924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7dfd7086f3671685e2cf1c1da72796d7</td>\n",
              "      <td>de</td>\n",
              "      <td>die/die&lt;det&gt;&lt;def&gt;&lt;f&gt;&lt;sg&gt;&lt;nom&gt;</td>\n",
              "      <td>die</td>\n",
              "      <td>die</td>\n",
              "      <td>det</td>\n",
              "      <td>[def, f, sg, nom]</td>\n",
              "      <td>3</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>die</td>\n",
              "      <td>die</td>\n",
              "      <td>(2, the)</td>\n",
              "      <td>2</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>the</td>\n",
              "      <td>0.001070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>35a54c25a2cda8127343f6a82e6f6b7d</td>\n",
              "      <td>de</td>\n",
              "      <td>mann/mann&lt;n&gt;&lt;m&gt;&lt;sg&gt;&lt;nom&gt;</td>\n",
              "      <td>mann</td>\n",
              "      <td>mann</td>\n",
              "      <td>n</td>\n",
              "      <td>[m, sg, nom]</td>\n",
              "      <td>4</td>\n",
              "      <td>husband</td>\n",
              "      <td>husband</td>\n",
              "      <td>mann</td>\n",
              "      <td>mann</td>\n",
              "      <td>(5, husband)</td>\n",
              "      <td>5</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>husband</td>\n",
              "      <td>1.258263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0cf63ffe3dda158bc3dbd55682b355ae</td>\n",
              "      <td>de</td>\n",
              "      <td>frau/frau&lt;n&gt;&lt;f&gt;&lt;sg&gt;&lt;nom&gt;</td>\n",
              "      <td>frau</td>\n",
              "      <td>frau</td>\n",
              "      <td>n</td>\n",
              "      <td>[f, sg, nom]</td>\n",
              "      <td>4</td>\n",
              "      <td>Mrs</td>\n",
              "      <td>Mrs</td>\n",
              "      <td>frau</td>\n",
              "      <td>frau</td>\n",
              "      <td>(3, mrs)</td>\n",
              "      <td>3</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>Mrs</td>\n",
              "      <td>6.309499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84920990d78044db53c1b012f5bf9ab5</td>\n",
              "      <td>de</td>\n",
              "      <td>das/das&lt;det&gt;&lt;def&gt;&lt;nt&gt;&lt;sg&gt;&lt;nom&gt;</td>\n",
              "      <td>das</td>\n",
              "      <td>das</td>\n",
              "      <td>det</td>\n",
              "      <td>[def, nt, sg, nom]</td>\n",
              "      <td>3</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>das</td>\n",
              "      <td>das</td>\n",
              "      <td>(3, the)</td>\n",
              "      <td>3</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>the</td>\n",
              "      <td>0.001070</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48bba5b6-5167-46d1-9940-602addadd890')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-48bba5b6-5167-46d1-9940-602addadd890 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-48bba5b6-5167-46d1-9940-602addadd890');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes['vectorlist'] = ''\n",
        "all_lexemes.loc[all_lexemes['learning_language']=='de','vectorlist'] = all_lexemes.loc[all_lexemes['learning_language']=='de','surface_form'].map(german_embeddings)"
      ],
      "metadata": {
        "id": "yUBC2RlaqDRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes.loc[all_lexemes['learning_language']=='de','vectorlist'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87tlFk4DrJ61",
        "outputId": "2b014472-f821-4bd9-e7e4-386378624e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [-0.12485239869, 0.403838020553, -0.3502961037...\n",
              "1    [0.0432688036856, -0.0914570974226, -0.1884847...\n",
              "2    [-0.0895643525954, 0.208577167333, -0.06067782...\n",
              "3    [0.0156379608103, 0.37910027723, -0.2013124868...\n",
              "4    [0.0388741864242, 0.160898776301, -0.105828861...\n",
              "Name: vectorlist, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes.loc[(all_lexemes['learning_language']=='de') & (all_lexemes['vectorlist'].isna()),:].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF-ByPooriTm",
        "outputId": "59afa9e3-9500-43a8-a6de-57e6bfeb9fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(355, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes.loc[(all_lexemes['learning_language']=='de') & (all_lexemes['vectorlist'].isna()),'vectorlist'] = all_lexemes.loc[(all_lexemes['learning_language']=='de') & (all_lexemes['vectorlist'].isna()),'lemma_form'].map(german_embeddings)"
      ],
      "metadata": {
        "id": "qoKJ_impuqNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes.loc[(all_lexemes['learning_language']=='de') & (all_lexemes['vectorlist'].isna()),:].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjxH3bwCvDsj",
        "outputId": "455e3046-49d2-4e1e-eb81-9214e9ae154a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(62, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "62 missing values for german embeddings. "
      ],
      "metadata": {
        "id": "MAzxIwbuvFUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes.loc[(all_lexemes['learning_language']=='de') & (all_lexemes['vectorlist'].isna()),:].shape"
      ],
      "metadata": {
        "id": "opjC1wkVuxe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e36a0361-3b24-43cd-b637-705a72d9190c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(62, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes.loc[all_lexemes['learning_language']=='en','vectorlist'] = all_lexemes.loc[all_lexemes['learning_language']=='en','surface_form'].map(english_embeddings)\n",
        "print(\"english embeddings suface form missing values\",all_lexemes.loc[(all_lexemes['learning_language']=='en') & (all_lexemes['vectorlist'].isna()),:].shape)\n",
        "all_lexemes.loc[(all_lexemes['learning_language']=='en') & (all_lexemes['vectorlist'].isna()),'vectorlist'] = all_lexemes.loc[(all_lexemes['learning_language']=='en') & (all_lexemes['vectorlist'].isna()),'lemma_form'].map(english_embeddings)\n",
        "print(\"english embeddings lemma+surface form missing values\",all_lexemes.loc[(all_lexemes['learning_language']=='en') & (all_lexemes['vectorlist'].isna()),:].shape)\n",
        "print(\"Imputing remaining missing values with centroid:\")\n",
        "\n",
        "all_lexemes.loc[all_lexemes['learning_language']=='es','vectorlist'] = all_lexemes.loc[all_lexemes['learning_language']=='es','surface_form'].map(spanish_embeddings)\n",
        "print(\"spanish embeddings suface form missing values\",all_lexemes.loc[(all_lexemes['learning_language']=='es') & (all_lexemes['vectorlist'].isna()),:].shape)\n",
        "all_lexemes.loc[(all_lexemes['learning_language']=='es') & (all_lexemes['vectorlist'].isna()),'vectorlist'] = all_lexemes.loc[(all_lexemes['learning_language']=='es') & (all_lexemes['vectorlist'].isna()),'lemma_form'].map(spanish_embeddings)\n",
        "print(\"spanish embeddings lemma+surface form missing values\",all_lexemes.loc[(all_lexemes['learning_language']=='es') & (all_lexemes['vectorlist'].isna()),:].shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVvrgvJ7rb8t",
        "outputId": "458583cf-38ba-45b0-f7ac-287a9ad744d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "english embeddings suface form missing values (734, 18)\n",
            "english embeddings lemma+surface form missing values (0, 18)\n",
            "Imputing remaining missing values with centroid:\n",
            "spanish embeddings suface form missing values (335, 18)\n",
            "spanish embeddings lemma+surface form missing values (6, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes.loc[all_lexemes['learning_language']=='fr','vectorlist'] = all_lexemes.loc[all_lexemes['learning_language']=='fr','surface_form'].map(french_embeddings)\n",
        "print(\"french embeddings suface form missing values\",all_lexemes.loc[(all_lexemes['learning_language']=='fr') & (all_lexemes['vectorlist'].isna()),:].shape)\n",
        "all_lexemes.loc[(all_lexemes['learning_language']=='fr') & (all_lexemes['vectorlist'].isna()),'vectorlist'] = all_lexemes.loc[(all_lexemes['learning_language']=='fr') & (all_lexemes['vectorlist'].isna()),'lemma_form'].map(french_embeddings)\n",
        "print(\"french embeddings lemma+surface form missing values\",all_lexemes.loc[(all_lexemes['learning_language']=='fr') & (all_lexemes['vectorlist'].isna()),:].shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvLm_pg1wOvD",
        "outputId": "9919b6cb-8aae-4f51-c41f-8b74f633055d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "french embeddings suface form missing values (1551, 18)\n",
            "french embeddings lemma+surface form missing values (38, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes.loc[all_lexemes['learning_language']=='it','vectorlist'] = all_lexemes.loc[all_lexemes['learning_language']=='it','surface_form'].map(italian_embeddings)\n",
        "print(\"italian embeddings suface form missing values\",all_lexemes.loc[(all_lexemes['learning_language']=='it') & (all_lexemes['vectorlist'].isna()),:].shape)\n",
        "all_lexemes.loc[(all_lexemes['learning_language']=='it') & (all_lexemes['vectorlist'].isna()),'vectorlist'] = all_lexemes.loc[(all_lexemes['learning_language']=='it') & (all_lexemes['vectorlist'].isna()),'lemma_form'].map(italian_embeddings)\n",
        "print(\"italian embeddings lemma+surface form missing values\",all_lexemes.loc[(all_lexemes['learning_language']=='it') & (all_lexemes['vectorlist'].isna()),:].shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf7xa_z7ws7a",
        "outputId": "2e4f7365-0f7d-4260-f1ff-f77fe08e27a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "italian embeddings suface form missing values (1138, 18)\n",
            "italian embeddings lemma+surface form missing values (23, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes.loc[all_lexemes['learning_language']=='pt','vectorlist'] = all_lexemes.loc[all_lexemes['learning_language']=='pt','surface_form'].map(portuguese_embeddings)\n",
        "print(\"portuguese embeddings suface form missing values\",all_lexemes.loc[(all_lexemes['learning_language']=='pt') & (all_lexemes['vectorlist'].isna()),:].shape)\n",
        "all_lexemes.loc[(all_lexemes['learning_language']=='pt') & (all_lexemes['vectorlist'].isna()),'vectorlist'] = all_lexemes.loc[(all_lexemes['learning_language']=='pt') & (all_lexemes['vectorlist'].isna()),'lemma_form'].map(portuguese_embeddings)\n",
        "print(\"portuguese embeddings lemma+surface form missing values\",all_lexemes.loc[(all_lexemes['learning_language']=='pt') & (all_lexemes['vectorlist'].isna()),:].shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VWfMNgUxY-Q",
        "outputId": "97d83588-a9ef-48b0-f761-124dec1082ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "portuguese embeddings suface form missing values (1759, 18)\n",
            "portuguese embeddings lemma+surface form missing values (47, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes.shape"
      ],
      "metadata": {
        "id": "U-EQF9IewL3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c28b40b0-5d8f-4e42-8511-9e1221d3b76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19279, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8vp0D0goPv2",
        "outputId": "c7897039-41dc-486f-a7df-cbbad8eff00d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['lexeme_id', 'learning_language', 'lexeme_string', 'surface_form',\n",
              "       'lemma_form', 'pos', 'modstrings', 'sf_length', 'sf_translation',\n",
              "       'lf_translation', 'surface_form_no_accents', 'lemma_form_no_accents',\n",
              "       'L_dist_word_tup_sf_noaccents', 'L_dist_sf_noaccents',\n",
              "       'L_dist_sf_noaccents_norm', 'IDFword', 'EnglishIDF', 'vectorlist'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes = all_lexemes.drop(['learning_language', 'lexeme_string', 'surface_form',\n",
        "       'lemma_form', 'pos', 'modstrings', 'sf_length', 'sf_translation',\n",
        "       'lf_translation', 'surface_form_no_accents', 'lemma_form_no_accents',\n",
        "       'L_dist_word_tup_sf_noaccents', 'L_dist_sf_noaccents',\n",
        "       'L_dist_sf_noaccents_norm', 'IDFword', 'EnglishIDF'],axis=1)"
      ],
      "metadata": {
        "id": "BzCgcvEToUeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk44IUc0oc4U",
        "outputId": "0060b42e-16b7-4018-9bf7-5d3e8c6d91ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['lexeme_id', 'vectorlist'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_pickle(path_name+\"Duolingo_wordvectors\",all_lexemes)"
      ],
      "metadata": {
        "id": "oshYBCayn9tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes[all_lexemes['vectorlist'].isna()].shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO9cinJcolEY",
        "outputId": "71f6ce10-7f97-4f62-e4d4-c4b2c82584df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "176"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 176 lexemes with no word vectors values out of 19,279. \n",
        "\n",
        "The lexemes in the train sets will be used to impute the test sets. "
      ],
      "metadata": {
        "id": "_GhC2Y2korXe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZTx7e6JjuCST"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}