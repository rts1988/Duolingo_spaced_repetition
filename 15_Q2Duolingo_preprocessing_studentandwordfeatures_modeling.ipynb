{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15_Q2Duolingo_preprocessing_studentandwordfeatures_modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6VA9cjJsr5VR"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMd0tkt/pt/srf858OHv0Mc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rts1988/Duolingo_spaced_repetition/blob/main/15_Q2Duolingo_preprocessing_studentandwordfeatures_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'cornflowerblue' size=4>Introduction</font>\n",
        "\n",
        "In the previous notebook student based features were explored, and some baselines were obtained by modeling using only student based features. (~30% precision at 30% recall).\n",
        "\n",
        "In this notebook, word based features from Q1 (derived word features, one-hot encoding + language dummies + word vectors) are added to the dataset.\n",
        "\n",
        "Along with the Q2 training set, the Q2 test set for unseen students and Q3 test sets of new languages will also be preprocessed, so that the preprocessing pipeline is kept the same. \n",
        "\n",
        "Classical, ensemble and neural nets will be used to optimize for precision at a recall of 30%. \n",
        "\n",
        "Lastly, the best model obtained is saved, and tested on the q2 test set (unseen students) and the q3 test sets (new languages)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hQgIm9UrfUWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'cornflowerblue' size=4>Loading data and computing aggregates</font>"
      ],
      "metadata": {
        "id": "6VA9cjJsr5VR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TRtS05yfEEO",
        "outputId": "192cde6b-41af-413f-d6d3-9e401f45f00c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "import bz2\n",
        "import pickle\n",
        "import _pickle as cPickle\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def decompress_pickle(file):\n",
        " data = bz2.BZ2File(file, 'rb')\n",
        " data = cPickle.load(data)\n",
        " return data\n",
        "\n",
        "def compressed_pickle(title, data):  # do not add extension in filename\n",
        " with bz2.BZ2File(title + '.pbz2', 'w') as f: \n",
        "  cPickle.dump(data, f)\n",
        "\n",
        "path_name = '/content/drive/MyDrive/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading q2train dataset:"
      ],
      "metadata": {
        "id": "Dx0KWvEsEzkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading q2 train set with student features based on 11 days history\n",
        "q2train11d_sfonly = decompress_pickle(path_name+\"Q2trainX11dayhist.pbz2\")\n",
        "\n",
        "# Loading q2 test set with student features based on 11 days history\n",
        "q2test11d_sfonly = decompress_pickle(path_name+\"Q2test11dayhist.pbz2\")\n",
        "\n",
        "# Loading q3 test english to german with 11 day history based student features\n",
        "q3test11d_en_to_de_sfonly = decompress_pickle(path_name+\"Q3test_en_to_de_11dayhist.pbz2\")\n",
        "\n",
        "# Loading q3 test italian to english with 11 day history based student features\n",
        "q3test11d_it_to_en_sfonly = decompress_pickle(path_name+\"Q3test_it_to_en_11dayhist.pbz2\")"
      ],
      "metadata": {
        "id": "US96OsjdN2tJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2train11d_sfonly.shape, q2test11d_sfonly.shape, q3test11d_en_to_de_sfonly.shape, q3test11d_it_to_en_sfonly.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKeE5lOcOCa4",
        "outputId": "69698ebf-2277-442a-c869-f71be830b505"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25753, 25), (3913, 25), (3212, 25), (1417, 25))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q2train11d_sfonly.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKDA89wkPF9M",
        "outputId": "ed4cee57-4f66-4bf6-bdbf-a8868538244d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['timestamp', 'delta', 'user_id', 'learning_language', 'ui_language',\n",
              "       'lexeme_id', 'lexeme_string', 'history_seen', 'history_correct',\n",
              "       'session_seen', 'session_correct', 'p_forgot_bin', 'simoverdiff',\n",
              "       'lang_frozenset', 'Datetime', 'delta_days', 'history_frac', 'Date_x',\n",
              "       'user_date_tup_x', 'Date_y', 'user_date_tup_y', 'avgp_forgot_day',\n",
              "       'avg_history_frac', 'numwordspracticed_day', 'avgdelta_day'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above dataframes have the student features already. \n",
        "\n",
        "Word based features will be added to it based on the lexeme id. "
      ],
      "metadata": {
        "id": "ZwaVh7EXPKOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Range of Dates is from Mar 10 -Mar 12, 2013, since we used 10 days of history from the dataset. "
      ],
      "metadata": {
        "id": "B1zQSmw2tcOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q2train11d_sfonly['Date_x'].min(), q2train11d_sfonly['Date_x'].max()"
      ],
      "metadata": {
        "id": "_DF-zuPMF6Rj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d2217a7-4baa-4dd8-a1b3-b82df4ac2be1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(datetime.date(2013, 3, 10), datetime.date(2013, 3, 12))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='cornflowerblue' size=4>Preprocessing all sets</font>\n",
        "\n",
        "1. Splitting Q2 training set into training and validation 95-5 set so that validation contains some unseen students. \n",
        "2. Adding the word based features from Q1 to the following sets:\n",
        "- q2 train\n",
        "- q2 validation\n",
        "- q2 test set\n",
        "- q3 test English to German\n",
        "- q3 test Italian to English\n",
        "\n",
        "save X and y separately for modeling."
      ],
      "metadata": {
        "id": "IUc-KQ1YQDAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users_list = q2train11d_sfonly['user_id'].unique()\n",
        "\n",
        "usersq2test_list = q2test11d_sfonly['user_id'].unique()\n",
        "usersq3test_en_to_de_list = q3test11d_en_to_de_sfonly['user_id'].unique()\n",
        "usersq3test_it_to_en_list = q3test11d_it_to_en_sfonly['user_id'].unique()\n"
      ],
      "metadata": {
        "id": "1K1LkRpGQCxH"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the validation set should have some unseen students, the list of users is split in a 95-5 split first."
      ],
      "metadata": {
        "id": "_-YR-DXxQmaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "userstrain_list = np.random.choice(users_list,int(0.95*len(users_list)),replace=False) # sample from list without replacement\n",
        "usersvalid_list = list(set(users_list).difference(userstrain_list))"
      ],
      "metadata": {
        "id": "hKbAY9rIQ8lj"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the dataframe based on the users:\n",
        "q2train = q2trainX11d_sfonly.loc[q2train11d_sfonly['user_id'].isin(userstrain_list),:]\n",
        "\n",
        "\n",
        "q2valid = q2trainX11d_sfonly.loc[q2train11d_sfonly['user_id'].isin(usersvalid_list),:]\n",
        "\n",
        "\n",
        "# print shapes of the dataframes after splitting by users.\n",
        "q2train.shape, q2valid.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Guct4SUCQCtx",
        "outputId": "d1d39842-8771-48a9-ff7b-0ea8ddee8708"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24409, 25), (1344, 25))"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting fraction of positive class samples in training and validation sets:"
      ],
      "metadata": {
        "id": "0TYLhh68SHMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q2train['p_forgot_bin'].sum()/q2train.shape[0], q2valid['p_forgot_bin'].sum()/q2valid.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoCjDcjDQCqp",
        "outputId": "c458b1b0-da5f-4164-9f01-b2d8c2d643c3"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.15318120365438978, 0.11383928571428571)"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q2test11d_sfonly['p_forgot_bin'].sum()/q2test11d_sfonly.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exy7a6lvjQag",
        "outputId": "6f00f98a-cc49-4c65-e00a-212397c04915"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16994633273703041"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q3test11d_en_to_de_sfonly['p_forgot_bin'].sum()/q3test11d_en_to_de_sfonly.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdMhB2kvjjVn",
        "outputId": "9b4b54f8-0579-4d14-ad82-ac774a6ddeaa"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16220423412204235"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q3test11d_it_to_en_sfonly['p_forgot_bin'].sum()/q3test11d_it_to_en_sfonly.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv55uAOTjo4g",
        "outputId": "550117dc-7a82-474e-8e16-5edefa0a0841"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.29992942836979536"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The validation set, and the q3 Italian to English test set has a slightly different fraction of positive class samples. "
      ],
      "metadata": {
        "id": "UI5j30DJSQJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding word based features:"
      ],
      "metadata": {
        "id": "GirL5UI8SWme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes = decompress_pickle(path_name+\"Duolingo_all_lexemes.pbz2\")"
      ],
      "metadata": {
        "id": "2geWELAKQCni"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting lexemes list for each dataset\n",
        "lexemestrain_list = q2train['lexeme_id'].unique()\n",
        "lexemesvalid_list = q2valid['lexeme_id'].unique()\n",
        "\n",
        "lexemesq2test_list = q2test11d_sfonly['lexeme_id'].unique()\n",
        "lexemesq3test_en_to_de_list = q3test11d_en_to_de_sfonly['lexeme_id'].unique()\n",
        "lexemesq3test_it_to_en_list = q3test11d_it_to_en_sfonly['lexeme_id'].unique()"
      ],
      "metadata": {
        "id": "Ar6_hRdhlvFs"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2trainlexemes = all_lexemes.loc[all_lexemes['lexeme_id'].isin(lexemestrain_list),:]\n",
        "q2validlexemes = all_lexemes.loc[all_lexemes['lexeme_id'].isin(lexemesvalid_list),:]\n",
        "\n",
        "q2testlexemes = all_lexemes.loc[all_lexemes['lexeme_id'].isin(lexemesq2test_list),:]\n",
        "\n",
        "q3test_en_to_de_lexemes = all_lexemes.loc[all_lexemes['lexeme_id'].isin(lexemesq3test_en_to_de_list),:]\n",
        "\n",
        "q3test_it_to_en_lexemes = all_lexemes.loc[all_lexemes['lexeme_id'].isin(lexemesq3test_it_to_en_list),:]"
      ],
      "metadata": {
        "id": "SVw_VXs3QCkj"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputing null values of English IDF:"
      ],
      "metadata": {
        "id": "F-YHgsCukb5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q2trainlexemes['EnglishIDF'].isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CryVfPeOk5R_",
        "outputId": "42d1dae9-1170-4e74-9860-ba111657f81d"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As was done earlier, the median IDF value from the train list will be used to impute values for all the validation and test sets sa well. "
      ],
      "metadata": {
        "id": "QmL1-AXimGoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating median value from train set lexemes data\n",
        "medianIDF = q2trainlexemes['EnglishIDF'].median()\n",
        "\n",
        "# imputing all datasets with median IDF from training set. \n",
        "q2trainlexemes.loc[q2trainlexemes['EnglishIDF'].isna(),'EnglishIDF'] = medianIDF\n",
        "q2validlexemes.loc[q2validlexemes['EnglishIDF'].isna(),'EnglishIDF'] = medianIDF\n",
        "\n",
        "q2testlexemes.loc[q2testlexemes['EnglishIDF'].isna(),'EnglishIDF'] = medianIDF\n",
        "\n",
        "q3test_en_to_de_lexemes.loc[q3test_en_to_de_lexemes['EnglishIDF'].isna(),'EnglishIDF'] = medianIDF\n",
        "q3test_it_to_en_lexemes.loc[q3test_it_to_en_lexemes['EnglishIDF'].isna(),'EnglishIDF'] = medianIDF"
      ],
      "metadata": {
        "id": "_QiGSUiKmOUr"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding of categoricals:\n"
      ],
      "metadata": {
        "id": "maNSFNWwnZd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc_lang = OneHotEncoder(sparse=False,handle_unknown='ignore')\n",
        "enc_pos = OneHotEncoder(sparse = False,handle_unknown = 'ignore')\n",
        "enc_mods = OneHotEncoder(sparse=False,handle_unknown='ignore')\n",
        "\n",
        "# get one hot encoded part of speech:\n",
        "enc_pos.fit(np.array(q2trainlexemes['pos']).reshape(-1, 1))\n",
        "q2train_pos = pd.DataFrame(enc_pos.transform(np.array(q2trainlexemes['pos']).reshape(-1, 1)),index=q2trainlexemes.index)\n",
        "\n",
        "q2valid_pos = pd.DataFrame(enc_pos.transform(np.array(q2validlexemes['pos']).reshape(-1, 1)),index=q2validlexemes.index)\n",
        "\n",
        "q2test_pos = pd.DataFrame(enc_pos.transform(np.array(q2testlexemes['pos']).reshape(-1, 1)),index=q2testlexemes.index)\n",
        "\n",
        "q3test_en_to_de_pos = pd.DataFrame(enc_pos.transform(np.array(q3test_en_to_de_lexemes['pos']).reshape(-1, 1)),index=q3test_en_to_de_lexemes.index)\n",
        "q3test_it_to_en_pos = pd.DataFrame(enc_pos.transform(np.array(q3test_it_to_en_lexemes['pos']).reshape(-1, 1)),index=q3test_it_to_en_lexemes.index)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OT8Nh5XT94dZ"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get one hot encoded modstrings\n",
        "q2trainlexemes['modstrings'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xJtlMPRAxyM",
        "outputId": "10e45e8e-1e0a-4402-f018-15f9a4938f08"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7                []\n",
              "20          [m, pl]\n",
              "21    [pri, p1, sg]\n",
              "22          [m, sg]\n",
              "23               []\n",
              "Name: modstrings, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since modstrings are saved as a list of strings, some more processing needs to be done before passing to the one hot encoder. "
      ],
      "metadata": {
        "id": "dOLCdf5pA1SV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# explode values of list so a separate record is created for each element of the list. \n",
        "q2trainlexemes['modstrings'].explode()\n",
        "enc_mods.fit(np.array(q2trainlexemes['modstrings'].explode()).reshape(-1,1))\n",
        "\n",
        "# q2 train set mod one-hots\n",
        "q2train_mods_exploded =enc_mods.transform(np.array(q2trainlexemes['modstrings'].explode()).reshape(-1,1))\n",
        "q2train_modsdf_exploded = pd.DataFrame(q2train_mods_exploded, index = q2trainlexemes['modstrings'].explode().index)\n",
        "# group the exploded one-hot dataframe by the q1lexemes index, sum up the exploded records (since one word can have many modifiers in its list)\n",
        "q2train_modsdf = q2train_modsdf_exploded.groupby(q2train_modsdf_exploded.index).sum()\n",
        "\n",
        "# for q2 valid set:\n",
        "q2valid_mods_exploded =enc_mods.transform(np.array(q2validlexemes['modstrings'].explode()).reshape(-1,1))\n",
        "q2valid_modsdf_exploded = pd.DataFrame(q2valid_mods_exploded, index = q2validlexemes['modstrings'].explode().index)\n",
        "q2valid_modsdf = q2valid_modsdf_exploded.groupby(q2valid_modsdf_exploded.index).sum()\n",
        "\n",
        "# for q2 test\n",
        "q2test_mods_exploded =enc_mods.transform(np.array(q2testlexemes['modstrings'].explode()).reshape(-1,1))\n",
        "q2test_modsdf_exploded = pd.DataFrame(q2test_mods_exploded, index = q2testlexemes['modstrings'].explode().index)\n",
        "q2test_modsdf = q2test_modsdf_exploded.groupby(q2test_modsdf_exploded.index).sum()\n",
        "\n",
        "# for q3 test english to german:\n",
        "q3test_entode_mods_exploded =enc_mods.transform(np.array(q3test_en_to_de_lexemes['modstrings'].explode()).reshape(-1,1))\n",
        "q3test_entode_modsdf_exploded = pd.DataFrame(q3test_entode_mods_exploded, index = q3test_en_to_de_lexemes['modstrings'].explode().index)\n",
        "q3test_entode_modsdf = q3test_entode_modsdf_exploded.groupby(q3test_entode_modsdf_exploded.index).sum()\n",
        "\n",
        "# for q3 test italian to german:\n",
        "q3test_ittoen_mods_exploded =enc_mods.transform(np.array(q3test_it_to_en_lexemes['modstrings'].explode()).reshape(-1,1))\n",
        "q3test_ittoen_modsdf_exploded = pd.DataFrame(q3test_ittoen_mods_exploded, index = q3test_it_to_en_lexemes['modstrings'].explode().index)\n",
        "q3test_ittoen_modsdf = q3test_ittoen_modsdf_exploded.groupby(q3test_ittoen_modsdf_exploded.index).sum()\n"
      ],
      "metadata": {
        "id": "5MnQB_n9_7zf"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2train_modsdf.shape, q2trainlexemes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSf7EfBND8P7",
        "outputId": "ccb1231b-619d-4e05-a15b-6105a8886560"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4696, 60), (4696, 17))"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q2valid_modsdf.shape, q2validlexemes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4iNeDuGuxct",
        "outputId": "dc387821-566f-4411-db81-c93fcaa0ad83"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((716, 60), (716, 17))"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q3test_ittoen_modsdf.shape, q3test_it_to_en_lexemes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1RGFz8l2vhB",
        "outputId": "432099c6-6f5f-40de-c873-33ac4eadff58"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((640, 60), (640, 17))"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The summed shape of the mods dataframe is the same length as the q1trainlexemes dataframe. \n",
        "\n",
        "Now the language, pos and mod sparse columns are combined along with numerical word features to make the first feature set. "
      ],
      "metadata": {
        "id": "SuIB3cPXFUxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q2trainfeatureset1 = pd.concat([q2trainlexemes[['sf_length', 'L_dist_sf_noaccents_norm', 'EnglishIDF','lexeme_id']], \n",
        "                                q2train_pos, \n",
        "                                q2train_modsdf, \n",
        "                                ],axis=1)\n",
        "\n",
        "\n",
        "q2validfeatureset1 = pd.concat([q2validlexemes[['sf_length', 'L_dist_sf_noaccents_norm', 'EnglishIDF','lexeme_id']], \n",
        "                                q2valid_pos, \n",
        "                                q2valid_modsdf, \n",
        "                                ],axis=1)\n",
        "\n",
        "q2testfeatureset1 = pd.concat([q2testlexemes[['sf_length', 'L_dist_sf_noaccents_norm', 'EnglishIDF','lexeme_id']], \n",
        "                                q2test_pos, \n",
        "                                q2test_modsdf, \n",
        "                                ],axis=1)\n",
        "\n",
        "q3test_en_to_de_featureset1 = pd.concat([q3test_en_to_de_lexemes[['sf_length', 'L_dist_sf_noaccents_norm', 'EnglishIDF','lexeme_id']], \n",
        "                                q3test_en_to_de_pos, \n",
        "                                q3test_entode_modsdf, \n",
        "                                ],axis=1)\n",
        "\n",
        "q3test_it_to_en_featureset1 = pd.concat([q3test_it_to_en_lexemes[['sf_length', 'L_dist_sf_noaccents_norm', 'EnglishIDF','lexeme_id']], \n",
        "                                q3test_it_to_en_pos, \n",
        "                                q3test_ittoen_modsdf, \n",
        "                                ],axis=1)\n"
      ],
      "metadata": {
        "id": "w97c1rnUFinJ"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2trainfeatureset1.shape, q3test_it_to_en_featureset1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66xiW_TAGn9H",
        "outputId": "51eb9822-ea0d-4e5f-9651-2046bd578d70"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4696, 117), (640, 117))"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q2trainfeatureset1.columns = ['sf_length', 'L_dist_sf_noaccents_norm', 'EnglishIDF','lexeme_id']  + ['pos_' + c for c in list(enc_pos.get_feature_names_out())] + ['mod_' + c for c in list(enc_mods.get_feature_names_out())]\n",
        "q2validfeatureset1.columns = q2trainfeatureset1.columns\n",
        "q2testfeatureset1.columns = q2trainfeatureset1.columns\n",
        "q3test_en_to_de_featureset1.columns = q2trainfeatureset1.columns\n",
        "q3test_it_to_en_featureset1.columns = q2trainfeatureset1.columns"
      ],
      "metadata": {
        "id": "Jx4xT0Zx7_54"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(q2trainfeatureset1.columns)) - len(set(q2trainfeatureset1.columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L42lWP3J8WV0",
        "outputId": "568b6003-90da-4015-fc40-a58de6483c45"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q2trainfeatureset1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "Q1F5YrAW745w",
        "outputId": "c4281c66-d462-416a-bf25-6d6be2ff22c7"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    sf_length  L_dist_sf_noaccents_norm  EnglishIDF  \\\n",
              "7           4                       0.5    3.733996   \n",
              "20          5                       0.6   10.981924   \n",
              "21          3                       0.0    4.051187   \n",
              "22          5                       0.8    0.438409   \n",
              "23          1                       0.0    0.002856   \n",
              "\n",
              "                           lexeme_id  pos_x0_@adv:a_part  \\\n",
              "7   73eecb492ca758ddab5371cf7b5cca32                 0.0   \n",
              "20  c84476c460737d9fb905dca3d35ec995                 0.0   \n",
              "21  1a913f2ded424985b9c02d0436008511                 0.0   \n",
              "22  38b770e66595fea718366523b4f7db3f                 0.0   \n",
              "23  4bdb859f599fa07dd5eecdab0acc2d34                 0.0   \n",
              "\n",
              "    pos_x0_@adv:a_peu_pres  pos_x0_@adv:au_moins  pos_x0_@adv:en_general  \\\n",
              "7                      0.0                   0.0                     0.0   \n",
              "20                     0.0                   0.0                     0.0   \n",
              "21                     0.0                   0.0                     0.0   \n",
              "22                     0.0                   0.0                     0.0   \n",
              "23                     0.0                   0.0                     0.0   \n",
              "\n",
              "    pos_x0_@adv:por_favor  pos_x0_@adv:por_supuesto  ...  mod_x0_prs  \\\n",
              "7                     0.0                       0.0  ...         0.0   \n",
              "20                    0.0                       0.0  ...         0.0   \n",
              "21                    0.0                       0.0  ...         0.0   \n",
              "22                    0.0                       0.0  ...         0.0   \n",
              "23                    0.0                       0.0  ...         0.0   \n",
              "\n",
              "    mod_x0_qnt  mod_x0_ref  mod_x0_sg  mod_x0_sint  mod_x0_sp  mod_x0_subj  \\\n",
              "7          0.0         0.0        0.0          0.0        0.0          0.0   \n",
              "20         0.0         0.0        0.0          0.0        0.0          0.0   \n",
              "21         0.0         0.0        1.0          0.0        0.0          0.0   \n",
              "22         0.0         0.0        1.0          0.0        0.0          0.0   \n",
              "23         0.0         0.0        0.0          0.0        0.0          0.0   \n",
              "\n",
              "    mod_x0_sup  mod_x0_tn  mod_x0_nan  \n",
              "7          0.0        0.0         1.0  \n",
              "20         0.0        0.0         0.0  \n",
              "21         0.0        0.0         0.0  \n",
              "22         0.0        0.0         0.0  \n",
              "23         0.0        0.0         1.0  \n",
              "\n",
              "[5 rows x 117 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7b6fcf4b-d996-4bac-9cd6-9dfded39198a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sf_length</th>\n",
              "      <th>L_dist_sf_noaccents_norm</th>\n",
              "      <th>EnglishIDF</th>\n",
              "      <th>lexeme_id</th>\n",
              "      <th>pos_x0_@adv:a_part</th>\n",
              "      <th>pos_x0_@adv:a_peu_pres</th>\n",
              "      <th>pos_x0_@adv:au_moins</th>\n",
              "      <th>pos_x0_@adv:en_general</th>\n",
              "      <th>pos_x0_@adv:por_favor</th>\n",
              "      <th>pos_x0_@adv:por_supuesto</th>\n",
              "      <th>...</th>\n",
              "      <th>mod_x0_prs</th>\n",
              "      <th>mod_x0_qnt</th>\n",
              "      <th>mod_x0_ref</th>\n",
              "      <th>mod_x0_sg</th>\n",
              "      <th>mod_x0_sint</th>\n",
              "      <th>mod_x0_sp</th>\n",
              "      <th>mod_x0_subj</th>\n",
              "      <th>mod_x0_sup</th>\n",
              "      <th>mod_x0_tn</th>\n",
              "      <th>mod_x0_nan</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4</td>\n",
              "      <td>0.5</td>\n",
              "      <td>3.733996</td>\n",
              "      <td>73eecb492ca758ddab5371cf7b5cca32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>5</td>\n",
              "      <td>0.6</td>\n",
              "      <td>10.981924</td>\n",
              "      <td>c84476c460737d9fb905dca3d35ec995</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.051187</td>\n",
              "      <td>1a913f2ded424985b9c02d0436008511</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>5</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.438409</td>\n",
              "      <td>38b770e66595fea718366523b4f7db3f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002856</td>\n",
              "      <td>4bdb859f599fa07dd5eecdab0acc2d34</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 117 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b6fcf4b-d996-4bac-9cd6-9dfded39198a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7b6fcf4b-d996-4bac-9cd6-9dfded39198a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7b6fcf4b-d996-4bac-9cd6-9dfded39198a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding word vectors:"
      ],
      "metadata": {
        "id": "BxPCWoLrqOWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordvecs = decompress_pickle(path_name+\"Duolingo_wordvectors.pbz2\")"
      ],
      "metadata": {
        "id": "2S07PPTUp9VC"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2trainvecs = wordvecs.loc[wordvecs['lexeme_id'].isin(lexemestrain_list),:]\n",
        "\n",
        "# for non-na values\n",
        "notna = q2trainvecs.loc[~q2trainvecs['vectorlist'].isna(),:]\n",
        "q2trainvecs_notna_expanded = pd.DataFrame(notna['vectorlist'].tolist(),index = notna['lexeme_id'])\n",
        "\n",
        "\n",
        "# computing centroid for imputation of nulls \n",
        "q2trainvecs_centroid = q2trainvecs_notna_expanded.mean(axis=0)\n",
        "\n",
        "# separating out null valued rows\n",
        "is_na = q2trainvecs.loc[q2trainvecs['vectorlist'].isna(),:]\n",
        "is_na.index = is_na['lexeme_id']\n",
        "\n",
        "numnull = is_na.shape[0]\n",
        "\n",
        "# imputing with centroid\n",
        "q2trainvecs_na_expanded = pd.concat([pd.DataFrame(q2trainvecs_centroid).transpose()]*numnull,axis=0)\n",
        "q2trainvecs_na_expanded.index = is_na.index\n",
        "\n",
        "# joining imputed null with the rest, keeping index consistent\n",
        "q2trainvecs_expanded = pd.concat([q2trainvecs_na_expanded,q2trainvecs_notna_expanded],axis=0)\n",
        "\n",
        "# viewing first few rows:\n",
        "q2trainvecs_expanded.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "LtcMiwsdp9QY",
        "outputId": "b47ed3c0-a66b-49fb-f379-188edb498f9c"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       0         1         2         3    \\\n",
              "lexeme_id                                                                  \n",
              "5377c84560aaf45988067be11302d1d8  0.107671  0.154306 -0.167931 -0.034556   \n",
              "26133676f75f829992c823dd7402f9a3  0.107671  0.154306 -0.167931 -0.034556   \n",
              "3baca3559289e14345edc0c53f516033  0.107671  0.154306 -0.167931 -0.034556   \n",
              "883996f36cc186da5983b13384230a9f  0.107671  0.154306 -0.167931 -0.034556   \n",
              "0f2bda0e0fb7b053496a4c9402800bf3  0.107671  0.154306 -0.167931 -0.034556   \n",
              "\n",
              "                                       4         5         6         7    \\\n",
              "lexeme_id                                                                  \n",
              "5377c84560aaf45988067be11302d1d8  0.116771  0.022966 -0.026985 -0.094826   \n",
              "26133676f75f829992c823dd7402f9a3  0.116771  0.022966 -0.026985 -0.094826   \n",
              "3baca3559289e14345edc0c53f516033  0.116771  0.022966 -0.026985 -0.094826   \n",
              "883996f36cc186da5983b13384230a9f  0.116771  0.022966 -0.026985 -0.094826   \n",
              "0f2bda0e0fb7b053496a4c9402800bf3  0.116771  0.022966 -0.026985 -0.094826   \n",
              "\n",
              "                                       8         9    ...      290       291  \\\n",
              "lexeme_id                                             ...                      \n",
              "5377c84560aaf45988067be11302d1d8  0.075554  1.226188  ... -0.13612  0.056093   \n",
              "26133676f75f829992c823dd7402f9a3  0.075554  1.226188  ... -0.13612  0.056093   \n",
              "3baca3559289e14345edc0c53f516033  0.075554  1.226188  ... -0.13612  0.056093   \n",
              "883996f36cc186da5983b13384230a9f  0.075554  1.226188  ... -0.13612  0.056093   \n",
              "0f2bda0e0fb7b053496a4c9402800bf3  0.075554  1.226188  ... -0.13612  0.056093   \n",
              "\n",
              "                                       292       293       294       295  \\\n",
              "lexeme_id                                                                  \n",
              "5377c84560aaf45988067be11302d1d8 -0.112848 -0.060538 -0.014504 -0.002696   \n",
              "26133676f75f829992c823dd7402f9a3 -0.112848 -0.060538 -0.014504 -0.002696   \n",
              "3baca3559289e14345edc0c53f516033 -0.112848 -0.060538 -0.014504 -0.002696   \n",
              "883996f36cc186da5983b13384230a9f -0.112848 -0.060538 -0.014504 -0.002696   \n",
              "0f2bda0e0fb7b053496a4c9402800bf3 -0.112848 -0.060538 -0.014504 -0.002696   \n",
              "\n",
              "                                       296       297       298       299  \n",
              "lexeme_id                                                                 \n",
              "5377c84560aaf45988067be11302d1d8 -0.022266 -0.013571 -0.032485  0.046411  \n",
              "26133676f75f829992c823dd7402f9a3 -0.022266 -0.013571 -0.032485  0.046411  \n",
              "3baca3559289e14345edc0c53f516033 -0.022266 -0.013571 -0.032485  0.046411  \n",
              "883996f36cc186da5983b13384230a9f -0.022266 -0.013571 -0.032485  0.046411  \n",
              "0f2bda0e0fb7b053496a4c9402800bf3 -0.022266 -0.013571 -0.032485  0.046411  \n",
              "\n",
              "[5 rows x 300 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-94dab0ed-3aa3-435f-8a13-b38ff2775236\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lexeme_id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5377c84560aaf45988067be11302d1d8</th>\n",
              "      <td>0.107671</td>\n",
              "      <td>0.154306</td>\n",
              "      <td>-0.167931</td>\n",
              "      <td>-0.034556</td>\n",
              "      <td>0.116771</td>\n",
              "      <td>0.022966</td>\n",
              "      <td>-0.026985</td>\n",
              "      <td>-0.094826</td>\n",
              "      <td>0.075554</td>\n",
              "      <td>1.226188</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.13612</td>\n",
              "      <td>0.056093</td>\n",
              "      <td>-0.112848</td>\n",
              "      <td>-0.060538</td>\n",
              "      <td>-0.014504</td>\n",
              "      <td>-0.002696</td>\n",
              "      <td>-0.022266</td>\n",
              "      <td>-0.013571</td>\n",
              "      <td>-0.032485</td>\n",
              "      <td>0.046411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26133676f75f829992c823dd7402f9a3</th>\n",
              "      <td>0.107671</td>\n",
              "      <td>0.154306</td>\n",
              "      <td>-0.167931</td>\n",
              "      <td>-0.034556</td>\n",
              "      <td>0.116771</td>\n",
              "      <td>0.022966</td>\n",
              "      <td>-0.026985</td>\n",
              "      <td>-0.094826</td>\n",
              "      <td>0.075554</td>\n",
              "      <td>1.226188</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.13612</td>\n",
              "      <td>0.056093</td>\n",
              "      <td>-0.112848</td>\n",
              "      <td>-0.060538</td>\n",
              "      <td>-0.014504</td>\n",
              "      <td>-0.002696</td>\n",
              "      <td>-0.022266</td>\n",
              "      <td>-0.013571</td>\n",
              "      <td>-0.032485</td>\n",
              "      <td>0.046411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3baca3559289e14345edc0c53f516033</th>\n",
              "      <td>0.107671</td>\n",
              "      <td>0.154306</td>\n",
              "      <td>-0.167931</td>\n",
              "      <td>-0.034556</td>\n",
              "      <td>0.116771</td>\n",
              "      <td>0.022966</td>\n",
              "      <td>-0.026985</td>\n",
              "      <td>-0.094826</td>\n",
              "      <td>0.075554</td>\n",
              "      <td>1.226188</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.13612</td>\n",
              "      <td>0.056093</td>\n",
              "      <td>-0.112848</td>\n",
              "      <td>-0.060538</td>\n",
              "      <td>-0.014504</td>\n",
              "      <td>-0.002696</td>\n",
              "      <td>-0.022266</td>\n",
              "      <td>-0.013571</td>\n",
              "      <td>-0.032485</td>\n",
              "      <td>0.046411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883996f36cc186da5983b13384230a9f</th>\n",
              "      <td>0.107671</td>\n",
              "      <td>0.154306</td>\n",
              "      <td>-0.167931</td>\n",
              "      <td>-0.034556</td>\n",
              "      <td>0.116771</td>\n",
              "      <td>0.022966</td>\n",
              "      <td>-0.026985</td>\n",
              "      <td>-0.094826</td>\n",
              "      <td>0.075554</td>\n",
              "      <td>1.226188</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.13612</td>\n",
              "      <td>0.056093</td>\n",
              "      <td>-0.112848</td>\n",
              "      <td>-0.060538</td>\n",
              "      <td>-0.014504</td>\n",
              "      <td>-0.002696</td>\n",
              "      <td>-0.022266</td>\n",
              "      <td>-0.013571</td>\n",
              "      <td>-0.032485</td>\n",
              "      <td>0.046411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0f2bda0e0fb7b053496a4c9402800bf3</th>\n",
              "      <td>0.107671</td>\n",
              "      <td>0.154306</td>\n",
              "      <td>-0.167931</td>\n",
              "      <td>-0.034556</td>\n",
              "      <td>0.116771</td>\n",
              "      <td>0.022966</td>\n",
              "      <td>-0.026985</td>\n",
              "      <td>-0.094826</td>\n",
              "      <td>0.075554</td>\n",
              "      <td>1.226188</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.13612</td>\n",
              "      <td>0.056093</td>\n",
              "      <td>-0.112848</td>\n",
              "      <td>-0.060538</td>\n",
              "      <td>-0.014504</td>\n",
              "      <td>-0.002696</td>\n",
              "      <td>-0.022266</td>\n",
              "      <td>-0.013571</td>\n",
              "      <td>-0.032485</td>\n",
              "      <td>0.046411</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 300 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94dab0ed-3aa3-435f-8a13-b38ff2775236')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-94dab0ed-3aa3-435f-8a13-b38ff2775236 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-94dab0ed-3aa3-435f-8a13-b38ff2775236');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q2trainlexemesvecs = pd.merge(left = q2trainfeatureset1,\\\n",
        "                              right = q2trainvecs_expanded,left_on = 'lexeme_id',right_index=True)"
      ],
      "metadata": {
        "id": "TE215D0xp9MM"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2trainlexemesvecs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B3YgYfQqnGy",
        "outputId": "be609176-b036-45d4-ee4e-7d9f816b95ea"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4696, 417)"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeating this process with q2validation set:"
      ],
      "metadata": {
        "id": "8dlJI7XZpoJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def joinvectorstolexemes(lexemesvalid_list,q2validfeatureset1):\n",
        "\n",
        "  q2validvecs = wordvecs.loc[wordvecs['lexeme_id'].isin(lexemesvalid_list),:]\n",
        "  print(\"length of vectors dataframe: \",q2validvecs.shape[0])\n",
        "  # for non-na values\n",
        "  notna = q2validvecs.loc[~q2validvecs['vectorlist'].isna(),:]\n",
        "  q2validvecs_notna_expanded = pd.DataFrame(notna['vectorlist'].tolist(),index = notna['lexeme_id'])\n",
        "  print(\"length of not null vectors dataframe: \",q2validvecs_notna_expanded.shape[0])\n",
        "  # separating out null valued rows\n",
        "  is_na = q2validvecs.loc[q2validvecs['vectorlist'].isna(),:]\n",
        "  is_na.index = is_na['lexeme_id']\n",
        "\n",
        "  numnull = is_na.shape[0]\n",
        "  print(\"length of null vectors dataframe: \",numnull)\n",
        "\n",
        "  # imputing with centroid previously computed from training set. \n",
        "  if numnull!=0:\n",
        "    q2validvecs_na_expanded = pd.concat([pd.DataFrame(q2trainvecs_centroid).transpose()]*numnull,axis=0)\n",
        "    q2validvecs_na_expanded.index = is_na.index\n",
        "  else:\n",
        "    q2validvecs_na_expanded = pd.DataFrame()\n",
        "  # joining imputed null with the rest, keeping index consistent\n",
        "  \n",
        "  q2validvecs_expanded = pd.concat([q2validvecs_na_expanded,q2validvecs_notna_expanded],axis=0)\n",
        "  print(\"all vectors shape: \",q2validvecs_expanded.shape )\n",
        "  # viewing first few rows:\n",
        "\n",
        "  q2validlexemesvecs = pd.merge(left = q2validfeatureset1,\\\n",
        "                                right = q2validvecs_expanded,left_on = 'lexeme_id',right_index=True)\n",
        "  \n",
        "  return q2validlexemesvecs"
      ],
      "metadata": {
        "id": "yStyYCw8k5JI"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2validlexemesvecs = joinvectorstolexemes(lexemesvalid_list,q2validfeatureset1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIdcWOFOx7Gu",
        "outputId": "a9adacc4-7b79-451d-84a7-5a6bcbddd0a5"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of vectors dataframe:  716\n",
            "length of not null vectors dataframe:  716\n",
            "length of null vectors dataframe:  0\n",
            "all vectors shape:  (716, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q2validlexemesvecs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujc6TzWDx_jZ",
        "outputId": "cb89e7a6-1a69-467d-fc5e-9906c484c8a9"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(716, 417)"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeating process for q2 test set:"
      ],
      "metadata": {
        "id": "v1fucD9Txmqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q2testlexemesvecs = joinvectorstolexemes(lexemesq2test_list,q2testfeatureset1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNVF0GA0S4oh",
        "outputId": "941ff0a4-2459-42ea-c813-9208fc12d143"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of vectors dataframe:  1858\n",
            "length of not null vectors dataframe:  1856\n",
            "length of null vectors dataframe:  2\n",
            "all vectors shape:  (1858, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q2testlexemesvecs.shape, q2testfeatureset1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am-dEYQmS4l2",
        "outputId": "f2822301-480a-4ce1-8478-32ec3ef3958c"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1858, 417), (1858, 117))"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeating process for q3 test (en to de)"
      ],
      "metadata": {
        "id": "RDljBM0YyfFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q3test_en_to_de_lexemevecs = joinvectorstolexemes(lexemesq3test_en_to_de_list,q3test_en_to_de_featureset1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHcQFrnGS4jd",
        "outputId": "349b2571-f152-4312-c27a-a427d7f22f38"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of vectors dataframe:  1030\n",
            "length of not null vectors dataframe:  1016\n",
            "length of null vectors dataframe:  14\n",
            "all vectors shape:  (1030, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q3test_en_to_de_lexemevecs.shape, q3test_en_to_de_featureset1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozRYfEeKzW4I",
        "outputId": "8ebe74b7-15b2-4eab-e35a-21fd28ba3f25"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1030, 417), (1030, 117))"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeating process for Italian to English"
      ],
      "metadata": {
        "id": "5QVEVhSY1lJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q3test_it_to_en_lexemevecs = joinvectorstolexemes(lexemesq3test_it_to_en_list,q3test_it_to_en_featureset1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4WyCLGMzWxV",
        "outputId": "434f69dd-383d-411b-eec2-8012096f2baf"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of vectors dataframe:  640\n",
            "length of not null vectors dataframe:  640\n",
            "length of null vectors dataframe:  0\n",
            "all vectors shape:  (640, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q3test_it_to_en_lexemevecs.shape, q3test_it_to_en_featureset1.shape,len(lexemesq3test_it_to_en_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1RvSzAczhcF",
        "outputId": "ccda9f26-3012-4acf-f4a3-bab84f465c96"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((640, 417), (640, 117), 640)"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All sets have been preprocessed. Now we join with the student features dataset by 'lexeme_id'"
      ],
      "metadata": {
        "id": "XQ7iJ0sB2-VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q2train_allf = pd.merge(left = q2train,right = q2trainlexemesvecs, left_on = 'lexeme_id',right_on = 'lexeme_id',how=\"left\")"
      ],
      "metadata": {
        "id": "l-RsUnpU2WtL"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2train_allf.shape, q2train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bo6oWin_38DU",
        "outputId": "33e2e2c4-82fa-42c3-8995-0895ae2aff9f"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24409, 441), (24409, 25))"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q2train_allf.columns[135:147]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbQN6bu24p3m",
        "outputId": "76b2eb67-04c1-4015-c0aa-024aaadf4853"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['mod_x0_sint',   'mod_x0_sp', 'mod_x0_subj',  'mod_x0_sup',\n",
              "         'mod_x0_tn',  'mod_x0_nan',             0,             1,\n",
              "                   2,             3,             4,             5],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can get rid of some extraneous columns post joining with student features datasets. \n",
        "\n",
        "Deleting the following columns:\n",
        "1. timestamp\n",
        "2. user id\n",
        "3. learning language\n",
        "4. ui language\n",
        "5. lexeme id\n",
        "6. lexeme string\n",
        "7. history correct (since history_frac is there)\n",
        "8. session seen\n",
        "9. session correct (target variable)\n",
        "10. lang_frozenset\n",
        "11. Datetime\n",
        "12. delta_days\n",
        "13. Date_x\n",
        "14. 'user_date_tup_x', \n",
        "15. 'Date_y',\n",
        "16.  'user_date_tup_y',"
      ],
      "metadata": {
        "id": "24SKtZwD411e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q2train_allf = q2train_allf.drop(['timestamp','user_id','learning_language','ui_language','lexeme_id','lexeme_string','history_correct','session_seen','session_correct',\\\n",
        "                                  'lang_frozenset','Datetime','delta_days','Date_x','user_date_tup_x','Date_y','user_date_tup_y'],axis=1)\n",
        "\n",
        "q2train_allf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLtLOEcr4uK3",
        "outputId": "76a57ec9-4865-4a1b-fe85-f2373eb4e334"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24409, 425)"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q2train_allf.select_dtypes('object').shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAbMN90o7DaY",
        "outputId": "ec612afb-eb0a-425e-b796-fb64eb292bc9"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24409, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no non-numerical variables left. \n",
        "\n",
        "Separating out y and X, and saving to compressed pickle files. "
      ],
      "metadata": {
        "id": "22jEhQcx7IxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q2trainy_allf = q2train_allf['p_forgot_bin']\n",
        "q2trainX_allf = q2train_allf.drop('p_forgot_bin',axis=1)\n",
        "compressed_pickle(path_name+\"Q2TRAIN_ALLFX\",q2trainX_allf)\n",
        "compressed_pickle(path_name+\"Q2TRAIN_ALLFy\",q2trainy_allf)"
      ],
      "metadata": {
        "id": "Me8xxPlH7K-J"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def joinstudentfeatures_save(q2train,q2trainlexemesvecs,prefixfilename):\n",
        "  q2train_allf = pd.merge(left = q2train,right = q2trainlexemesvecs, left_on = 'lexeme_id',right_on = 'lexeme_id',how=\"left\")\n",
        "  print(\"Shape of all features dataset\")\n",
        "  print(\"Deleting extraneous columns\")\n",
        "  q2train_allf = q2train_allf.drop(['timestamp','user_id','learning_language','ui_language','lexeme_id','lexeme_string','history_correct','session_seen','session_correct',\\\n",
        "                                  'lang_frozenset','Datetime','delta_days','Date_x','user_date_tup_x','Date_y','user_date_tup_y'],axis=1)\n",
        "\n",
        "  print(\"New shape: \",q2train_allf.shape)\n",
        "\n",
        "  q2trainy_allf = q2train_allf['p_forgot_bin']\n",
        "  q2trainX_allf = q2train_allf.drop('p_forgot_bin',axis=1)\n",
        "\n",
        "  print(\"Checking for any nulls before saving\")\n",
        "  print(\"Number of null values\",q2trainX_allf.isna().any().sum())\n",
        "\n",
        "  compressed_pickle(path_name+prefixfilename+\"X\",q2trainX_allf)\n",
        "  compressed_pickle(path_name+prefixfilename+\"y\",q2trainy_allf)\n",
        "\n",
        "  print('saved: ',prefixfilename,\"X and y\")\n"
      ],
      "metadata": {
        "id": "VHOQVWT97xUR"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joinstudentfeatures_save(q2train,q2trainlexemesvecs,prefixfilename = 'Q2TRAIN_ALLF')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlUSuD8q8nnC",
        "outputId": "95f61781-d550-4f76-93e3-3a66ff02f235"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of all features dataset\n",
            "Deleting extraneous columns\n",
            "New shape:  (24409, 425)\n",
            "Checking for any nulls before saving\n",
            "Number of null values 0\n",
            "saved:  Q2TRAIN_ALLF X and y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joinstudentfeatures_save(q2valid,q2validlexemesvecs,prefixfilename = 'Q2VALID_ALLF')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aikH_bPw8xUk",
        "outputId": "849e67b9-2560-4d8f-ec26-1fcd5a43f314"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of all features dataset\n",
            "Deleting extraneous columns\n",
            "New shape:  (1344, 425)\n",
            "Checking for any nulls before saving\n",
            "Number of null values 0\n",
            "saved:  Q2VALID_ALLF X and y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joinstudentfeatures_save(q2test11d_sfonly,q2testlexemesvecs,prefixfilename = 'Q2TEST_ALLF')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McbnfpNH9JY2",
        "outputId": "e0b9f1f4-8819-439f-cb13-7a572376ec53"
      },
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of all features dataset\n",
            "Deleting extraneous columns\n",
            "New shape:  (3913, 425)\n",
            "Checking for any nulls before saving\n",
            "Number of null values 0\n",
            "saved:  Q2TEST_ALLF X and y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joinstudentfeatures_save(q3test11d_en_to_de_sfonly,q3test_en_to_de_lexemevecs,prefixfilename = 'Q3TESTENTODE_ALLF')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLzVscqw9iod",
        "outputId": "b6af2cb5-e1e2-4df1-ec11-67109d76592e"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of all features dataset\n",
            "Deleting extraneous columns\n",
            "New shape:  (3212, 425)\n",
            "Checking for any nulls before saving\n",
            "Number of null values 0\n",
            "saved:  Q3TESTENTODE_ALLF X and y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joinstudentfeatures_save(q3test11d_it_to_en_sfonly,q3test_it_to_en_lexemevecs,prefixfilename = 'Q3TESTITTOEN_ALLF')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiWnWQHe9iit",
        "outputId": "60ce205d-71f2-4953-c140-e0c63bfa4450"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of all features dataset\n",
            "Deleting extraneous columns\n",
            "New shape:  (1417, 425)\n",
            "Checking for any nulls before saving\n",
            "Number of null values 0\n",
            "saved:  Q3TESTITTOEN_ALLF X and y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'cornflowerblue' size = 4>Conclusions and Next Steps</font>\n",
        "\n",
        "Since the datasets for Q2 training, validation, test with unseen students are now prepared with word and student based features, modeling can be done. \n",
        "\n",
        "The best model will be selected for trial on the two unseen languages. "
      ],
      "metadata": {
        "id": "nn5tuktkBgky"
      }
    }
  ]
}