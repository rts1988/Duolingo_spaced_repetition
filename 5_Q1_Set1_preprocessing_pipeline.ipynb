{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_Q1_Set1_preprocessing_pipeline.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP5Vzd9JCQVaO/RKl9Mo3tm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rts1988/Duolingo_spaced_repetition/blob/main/5_Q1_Set1_preprocessing_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'cornflowerblue' size=4>Introduction</font>\n",
        "\n",
        "In this notebook, the lexeme strings and word features derived in the previous notebook are used as a feature set.\n",
        "\n",
        "Using only the lexeems in the Q1 training set, PCA is used to reduce the dimensions of the feature set. \n",
        "\n",
        "There is a choice on whether to use sparse matrices or do dimension reduction. PCA will make the reduced dimension set more dense. However, it will help reduce the curse of dimensionality. \n",
        "\n",
        "After scaling and dimension reduction of the word based features, the reduced features will be joined to the main Q1 training set as compressed sparse matrix. \n",
        "\n",
        "The same preprocessing pipeline will be used to transform the Q1test set. \n",
        "\n",
        "This will make both datasets ready for modeling. \n",
        "\n",
        "Since larger dimensions are being dealt with, high-RAM option is selected:"
      ],
      "metadata": {
        "id": "VlsUIwyQ5Dsr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUfGiXccKpih",
        "outputId": "183fd0c4-5e46-4b11-960f-5bc3d3fb4f44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries and mounting google drive:"
      ],
      "metadata": {
        "id": "KXHUqwRH6SUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bz2\n",
        "import pickle\n",
        "import _pickle as cPickle\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def decompress_pickle(file):\n",
        " data = bz2.BZ2File(file, 'rb')\n",
        " data = cPickle.load(data)\n",
        " return data\n",
        "\n",
        "def compressed_pickle(title, data):  # do not add extension in filename\n",
        " with bz2.BZ2File(title + '.pbz2', 'w') as f: \n",
        "  cPickle.dump(data, f)\n",
        "\n",
        "path_name = '/content/drive/MyDrive/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA8ZfcqKLCX9",
        "outputId": "5025caa2-865c-4613-95bf-11325d7dcbb0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Q1 train set:"
      ],
      "metadata": {
        "id": "zkV1ZUvZ6c-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1train = decompress_pickle(path_name+\"Q1TRAIN.pbz2\") "
      ],
      "metadata": {
        "id": "dtSXcCK2LLU0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Q1 test set:"
      ],
      "metadata": {
        "id": "J49yAIM6_eqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1test = decompress_pickle(path_name+\"Q1TEST.pbz2\")"
      ],
      "metadata": {
        "id": "3G8jzcA-_gEn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirming the shapes:"
      ],
      "metadata": {
        "id": "JwJIA6Fv6mnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1train.shape, q1test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJXD-QmwLNve",
        "outputId": "30becf16-9f31-4f04-8e95-eba8b09795d6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8070561, 17), (1795528, 17))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q1train.size/10**6, q1test.size/10**6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtF6GiR3Lgxd",
        "outputId": "eb288afb-be58-43b9-a607-9d6aa2a4ee8c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(137.199537, 30.523976)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of lexeme ids in q1train dataframe:"
      ],
      "metadata": {
        "id": "vIzfbEK57CQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1train_lexemelist = q1train['lexeme_id'].unique()"
      ],
      "metadata": {
        "id": "fbrcMnSj7EmP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes = decompress_pickle(path_name+\"Duolingo_all_lexemes.pbz2\")"
      ],
      "metadata": {
        "id": "DPG56uwYLPzE"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_lexemes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySoYJV_4Lcyz",
        "outputId": "3f75cbb0-f946-470e-e2b1-1db808adb070"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19279, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering to only the words in the training set "
      ],
      "metadata": {
        "id": "24M_qimM64r0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1trainlexemes = all_lexemes.loc[all_lexemes['lexeme_id'].isin(q1train_lexemelist),:]"
      ],
      "metadata": {
        "id": "KSFtS9sMLe6m"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1trainlexemes.columns, q1trainlexemes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rkDyqxT9C1J",
        "outputId": "3c989de7-ee98-4cc0-986c-1cb5ab9df416"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Index(['lexeme_id', 'learning_language', 'lexeme_string', 'surface_form',\n",
              "        'lemma_form', 'pos', 'modstrings', 'sf_length', 'sf_translation',\n",
              "        'lf_translation', 'surface_form_no_accents', 'lemma_form_no_accents',\n",
              "        'L_dist_word_tup_sf_noaccents', 'L_dist_sf_noaccents',\n",
              "        'L_dist_sf_noaccents_norm', 'IDFword', 'EnglishIDF'],\n",
              "       dtype='object'), (12446, 17))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering out test set lexemes dataframe for transformation:"
      ],
      "metadata": {
        "id": "lpwzVtVUIq_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1test_lexemelist = q1test['lexeme_id'].unique()\n",
        "q1testlexemes = all_lexemes.loc[all_lexemes['lexeme_id'].isin(q1test_lexemelist),:]"
      ],
      "metadata": {
        "id": "2u7AuHk6It7i"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1testlexemes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4evzTh5lI-OR",
        "outputId": "0f6dfbf3-fac5-4653-fcae-02fbe369e088"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2776, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'cornflowerblue' size=4>Preprocessing and PCA</font>"
      ],
      "metadata": {
        "id": "PJE84YdT8DEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'cornflowerblue' size=3>One hot encoding of categoricals</font>\n",
        "The learning language, pos and mostrings are further converted to binary dummies: "
      ],
      "metadata": {
        "id": "lA1zFUID7ZnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc_lang = OneHotEncoder(sparse=False,handle_unknown='ignore')\n",
        "enc_pos = OneHotEncoder(sparse = False,handle_unknown = 'ignore')\n",
        "enc_mods = OneHotEncoder(sparse=False,handle_unknown='ignore')\n",
        "\n",
        "# get one hot encoded learning language\n",
        "enc_lang.fit(np.array(q1trainlexemes['learning_language']).reshape(-1, 1))\n",
        "q1train_lang = pd.DataFrame(enc_lang.transform(np.array(q1trainlexemes['learning_language']).reshape(-1, 1)),index=q1trainlexemes.index)\n",
        "\n",
        "# get one hot encoded part of speech:\n",
        "enc_pos.fit(np.array(q1trainlexemes['pos']).reshape(-1, 1))\n",
        "q1train_pos = pd.DataFrame(enc_pos.transform(np.array(q1trainlexemes['pos']).reshape(-1, 1)),index=q1trainlexemes.index)\n"
      ],
      "metadata": {
        "id": "OT8Nh5XT94dZ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get one hot encoded modstrings\n",
        "q1trainlexemes['modstrings'].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xJtlMPRAxyM",
        "outputId": "03a25d7d-6ca2-46fc-8e97-4ae893a72931"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7                []\n",
              "20          [m, pl]\n",
              "21    [pri, p1, sg]\n",
              "22          [m, sg]\n",
              "24    [pri, p3, sg]\n",
              "Name: modstrings, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since modstrings are saved as a list of strings, some more processing needs to be done before passing to the one hot encoder. "
      ],
      "metadata": {
        "id": "dOLCdf5pA1SV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# explode values of list so a separate record is created for each element of the list. \n",
        "q1trainlexemes['modstrings'].explode()\n",
        "enc_mods.fit(np.array(q1trainlexemes['modstrings'].explode()).reshape(-1,1))\n",
        "q1train_mods_exploded =enc_mods.transform(np.array(q1trainlexemes['modstrings'].explode()).reshape(-1,1))\n",
        "\n",
        "q1train_modsdf_exploded = pd.DataFrame(q1train_mods_exploded, index = q1trainlexemes['modstrings'].explode().index)\n",
        "\n",
        "# group the exploded one-hot dataframe by the q1lexemes index, sum up the exploded records (since one word can have many modifiers in its list)\n",
        "q1train_modsdf = q1train_modsdf_exploded.groupby(q1train_modsdf_exploded.index).sum()\n",
        "#np.concatenate([np.array(q1trainlexemes['modstrings'].explode().index).reshape(-1,1),q1train_mods],axis=1)"
      ],
      "metadata": {
        "id": "5MnQB_n9_7zf"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1train_modsdf.shape, q1trainlexemes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSf7EfBND8P7",
        "outputId": "967ec547-0a8a-4510-f189-2056e035cff3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((12446, 86), (12446, 17))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The summed shape of the mods dataframe is the same length as the q1trainlexemes dataframe. \n",
        "\n",
        "Now the language, pos and mod sparse columns are combined along with numerical word features to make the first feature set. "
      ],
      "metadata": {
        "id": "SuIB3cPXFUxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1trainfeatureset1 = pd.concat([q1trainlexemes[['sf_length', 'L_dist_sf_noaccents_norm', 'EnglishIDF','lexeme_id']],\n",
        "                                q1train_lang, \n",
        "                                q1train_pos, \n",
        "                                q1train_modsdf, \n",
        "                                ],axis=1)"
      ],
      "metadata": {
        "id": "w97c1rnUFinJ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1trainfeatureset1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66xiW_TAGn9H",
        "outputId": "be039e2b-a07f-49c4-c568-e43d12c8f311"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12446, 167)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature set has 167 columns. \n",
        "\n",
        "Applying the same transformations to the test set lexemes:"
      ],
      "metadata": {
        "id": "0w9M0GziG5RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transforming the language column based on already fit one hot encoder:\n",
        "q1test_lang = pd.DataFrame(enc_lang.transform(np.array(q1testlexemes['learning_language']).reshape(-1, 1)),index=q1testlexemes.index)\n",
        "\n",
        "# get one hot encoded part of speech:\n",
        "q1test_pos = pd.DataFrame(enc_pos.transform(np.array(q1testlexemes['pos']).reshape(-1, 1)),index=q1testlexemes.index)\n"
      ],
      "metadata": {
        "id": "dGIkcT4BJVX_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# explode values of list so a separate record is created for each element of the list. \n",
        "q1testlexemes['modstrings'].explode()\n",
        "#enc_mods.fit(np.array(q1trainlexemes['modstrings'].explode()).reshape(-1,1))\n",
        "q1test_mods_exploded =enc_mods.transform(np.array(q1testlexemes['modstrings'].explode()).reshape(-1,1))\n",
        "\n",
        "q1test_modsdf_exploded = pd.DataFrame(q1test_mods_exploded, index = q1testlexemes['modstrings'].explode().index)\n",
        "\n",
        "# group the exploded one-hot dataframe by the q1lexemes index, sum up the exploded records (since one word can have many modifiers in its list)\n",
        "q1test_modsdf = q1test_modsdf_exploded.groupby(q1test_modsdf_exploded.index).sum()\n",
        "#np.concatenate([np.array(q1trainlexemes['modstrings'].explode().index).reshape(-1,1),q1train_mods],axis=1)"
      ],
      "metadata": {
        "id": "RB4SE2EPJVYB"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1test_modsdf.shape, q1testlexemes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc7022a-2672-430c-9415-dc5da857100f",
        "id": "14mK8h_PJVYB"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2776, 86), (2776, 17))"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The summed shape of the mods dataframe is the same length as the q1trainlexemes dataframe. \n",
        "\n",
        "Now the language, pos and mod sparse columns are combined along with numerical word features to make the first feature set. "
      ],
      "metadata": {
        "id": "DXgOm8y0JVYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1testfeatureset1 = pd.concat([q1testlexemes[['lexeme_id','sf_length', 'L_dist_sf_noaccents_norm', 'EnglishIDF']],\n",
        "                                q1test_lang, \n",
        "                                q1test_pos, \n",
        "                                q1test_modsdf, \n",
        "                                ],axis=1)"
      ],
      "metadata": {
        "id": "GtDW4vRMJVYB"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1testfeatureset1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d52daef5-e263-491b-d6c8-8a29d3d04c42",
        "id": "H2OecOY0JVYB"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2776, 167)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of dimensions matches for train and test post binarizing categorical columns. They are saved as compressed pickle files. "
      ],
      "metadata": {
        "id": "WFoRQubnLblE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_pickle(path_name+\"Q1TRAIN_lexemesFS1\",q1trainfeatureset1)\n",
        "compressed_pickle(path_name+\"Q1TEST_lexemesFS1\",q1testfeatureset1)"
      ],
      "metadata": {
        "id": "WmmZmhPZLjrk"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plan\n",
        "\n",
        "Pipe 1: (no PCA, just combined)\n",
        "Featureset1 -> join with main dataframe -> convert to sparse matrix -> minmaxscaler\n",
        "\n",
        "Pipe2: (PCA of lexeme features)\n",
        "Featureset1 -> stdscaler -> PCA (explained_variance=0.9) -> join with main dataframe (after scaling with stdscaler) -> convert to sparse matrix\n"
      ],
      "metadata": {
        "id": "HNv2DdUAIjG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA, SparsePCA\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
      ],
      "metadata": {
        "id": "U9s_8TN58Ciq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estimate of total size of dataset once joined. "
      ],
      "metadata": {
        "id": "qLaQu-zkLwMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(q1trainfeatureset1.size/10**6)/q1trainfeatureset1.shape[0]*q1train.shape[0] + q1train.size/10**6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CT573OULmzp",
        "outputId": "288611e3-06fb-4dc4-88f6-04445cc584fd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1417.60566"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The estimated size of a full combined feature set without any dimension reduction is 1.4GB. \n",
        "\n",
        "Attempts to combine it all at once failed even with high RAM.\n",
        "\n",
        "```\n",
        "# getting lexeme id for the full feature set 1\n",
        "q1trainfeatureset1['lexeme_id'] = q1trainlexemes.loc[q1trainfeatureset1.index,'lexeme_id']\n",
        "\n",
        "# joining with main dataframe on lexeme id\n",
        "q1train_withfs1 = pd.merge(left= q1train,right=q1train,left_on='lexeme_id',right_on = 'lexeme_id',how=\"left\")\n",
        "```"
      ],
      "metadata": {
        "id": "pGkVOi0DMJJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a workaround, the following process is used.\n",
        "\n",
        "1.  100,000 rows of the main dataframe are taken at a time,joined with q1trainfeatureset1, \n",
        "2. and then compressed to a sparse matrix. \n",
        "3. The sparse matrices generated are stacked vertically to build a compressed sprarse version of the full dataset. "
      ],
      "metadata": {
        "id": "Irj2M4afLzll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Droping extraneous columns from q1train, and q1trainfeatureset1"
      ],
      "metadata": {
        "id": "sPAvRSs8ONNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qRFSTdXOxy4",
        "outputId": "6d7d97b4-4b1a-49f3-f526-0d3e9a7e9ec4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['timestamp', 'delta', 'user_id', 'learning_language', 'ui_language',\n",
              "       'lexeme_id', 'lexeme_string', 'history_seen', 'history_correct',\n",
              "       'session_seen', 'session_correct', 'p_forgot_bin', 'simoverdiff',\n",
              "       'lang_frozenset', 'Datetime', 'delta_days', 'history_frac'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The columns needed for merging with lexemes for q1train_X: delta, lexeme_id, history_seen, history_frac, simoverdiff, \n",
        "\n",
        "The columns needed for q1train_y: p_forgot_bin\n"
      ],
      "metadata": {
        "id": "6NG3Yrs8PLUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1train_X = q1train[['lexeme_id','delta','history_seen','history_frac','simoverdiff']]\n",
        "\n",
        "q1train_y = q1train['p_forgot_bin']"
      ],
      "metadata": {
        "id": "043tTOlkPKtV"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1train_X.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXkr7CctnCV4",
        "outputId": "005aa1f1-1b1a-42ee-8ae9-d370d0e2576f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['lexeme_id', 'delta', 'history_seen', 'history_frac', 'simoverdiff'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the exception of lexeme id (needed for joining) all other non-numerical columns have been dropped. "
      ],
      "metadata": {
        "id": "u3d2Oi8fnFz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1trainfeatureset1.columns[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oNKExg0mRzI",
        "outputId": "99b6691f-cb5b-48fd-cbdb-a239a37d93bd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sf_length', 'L_dist_sf_noaccents_norm', 'EnglishIDF', 0, 1], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified not to save files."
      ],
      "metadata": {
        "id": "tEWCk9IRjoTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try: \n",
        "  del Xq1\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "RSM52LpPoMqZ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import coo_matrix, vstack\n",
        "count = 0\n",
        "for i in range(0,q1train_X.shape[0],100000):\n",
        "  if count%2:\n",
        "    print(count)\n",
        "  \n",
        "  subdata = q1train_X.iloc[i:min(i+100000,q1train_X.shape[0]),:]\n",
        "  subdata = pd.merge(left = subdata, right = q1trainfeatureset1,left_on = 'lexeme_id',right_on = 'lexeme_id',how=\"left\")\n",
        "  subdata = subdata.drop('lexeme_id',axis=1)\n",
        "  #print('size: ',subdata.size/10**6)\n",
        "\n",
        "  mat = coo_matrix(subdata)\n",
        "  #print('size after compression: ',mat.size/10**6)\n",
        "  if i >1:\n",
        "    print('stacking: ')\n",
        "    Xq1 = vstack([Xq1,mat])\n",
        "    #print('saving: ',\"Xq1_\"+str(count))\n",
        "    #compressed_pickle(path_name+\"Xq1_\"+str(count), mat)\n",
        "  else:\n",
        "    Xq1 = mat\n",
        "    #compressed_pickle(path_name+\"Xq1_\"+str(count), mat)\n",
        "  del subdata, mat\n",
        "  count +=1\n",
        "#print('Saved all, size Xq1: ',Xq1.size/10**6)\n",
        "compressed_pickle(path_name+\"q1train_pipe1\", Xq1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6XgbdsULtpU",
        "outputId": "414914f5-66b8-4924-8bce-2202c9b00b50"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "stacking: \n",
            "stacking: \n",
            "3\n",
            "stacking: \n",
            "stacking: \n",
            "5\n",
            "stacking: \n",
            "stacking: \n",
            "7\n",
            "stacking: \n",
            "stacking: \n",
            "9\n",
            "stacking: \n",
            "stacking: \n",
            "11\n",
            "stacking: \n",
            "stacking: \n",
            "13\n",
            "stacking: \n",
            "stacking: \n",
            "15\n",
            "stacking: \n",
            "stacking: \n",
            "17\n",
            "stacking: \n",
            "stacking: \n",
            "19\n",
            "stacking: \n",
            "stacking: \n",
            "21\n",
            "stacking: \n",
            "stacking: \n",
            "23\n",
            "stacking: \n",
            "stacking: \n",
            "25\n",
            "stacking: \n",
            "stacking: \n",
            "27\n",
            "stacking: \n",
            "stacking: \n",
            "29\n",
            "stacking: \n",
            "stacking: \n",
            "31\n",
            "stacking: \n",
            "stacking: \n",
            "33\n",
            "stacking: \n",
            "stacking: \n",
            "35\n",
            "stacking: \n",
            "stacking: \n",
            "37\n",
            "stacking: \n",
            "stacking: \n",
            "39\n",
            "stacking: \n",
            "stacking: \n",
            "41\n",
            "stacking: \n",
            "stacking: \n",
            "43\n",
            "stacking: \n",
            "stacking: \n",
            "45\n",
            "stacking: \n",
            "stacking: \n",
            "47\n",
            "stacking: \n",
            "stacking: \n",
            "49\n",
            "stacking: \n",
            "stacking: \n",
            "51\n",
            "stacking: \n",
            "stacking: \n",
            "53\n",
            "stacking: \n",
            "stacking: \n",
            "55\n",
            "stacking: \n",
            "stacking: \n",
            "57\n",
            "stacking: \n",
            "stacking: \n",
            "59\n",
            "stacking: \n",
            "stacking: \n",
            "61\n",
            "stacking: \n",
            "stacking: \n",
            "63\n",
            "stacking: \n",
            "stacking: \n",
            "65\n",
            "stacking: \n",
            "stacking: \n",
            "67\n",
            "stacking: \n",
            "stacking: \n",
            "69\n",
            "stacking: \n",
            "stacking: \n",
            "71\n",
            "stacking: \n",
            "stacking: \n",
            "73\n",
            "stacking: \n",
            "stacking: \n",
            "75\n",
            "stacking: \n",
            "stacking: \n",
            "77\n",
            "stacking: \n",
            "stacking: \n",
            "79\n",
            "stacking: \n",
            "stacking: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xq1.size/10**6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_bWErwvp8Y8",
        "outputId": "353169db-26a5-4cee-c9c8-bd2609326ccc"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86.851987"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The compressed matrix X is 86.85 MB.\n",
        "\n",
        "Saving q1train_y:"
      ],
      "metadata": {
        "id": "89YkXfsbqC1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_pickle(path_name+\"q1train_y\",q1train_y)"
      ],
      "metadata": {
        "id": "suZ5GotDqqfB"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1test_X = q1test[['lexeme_id','delta','history_seen','history_frac','simoverdiff']]\n",
        "\n",
        "q1test_y = q1test['p_forgot_bin']"
      ],
      "metadata": {
        "id": "NxKcnKqUqdCN"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1test_X.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cea4668-7078-4ae6-8890-b5fcc03b0bf4",
        "id": "ZyT9WN-lqdCO"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['lexeme_id', 'delta', 'history_seen', 'history_frac', 'simoverdiff'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the exception of lexeme id (needed for joining) all other non-numerical columns have been dropped. "
      ],
      "metadata": {
        "id": "FOkXktWGqdCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1testfeatureset1.columns[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8792e9-6ccb-46bc-ecca-1c4c252eafde",
        "id": "4rZDR4trqdCO"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['lexeme_id', 'sf_length', 'L_dist_sf_noaccents_norm', 'EnglishIDF', 0], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modified not to save files."
      ],
      "metadata": {
        "id": "j5qKBJ2zqdCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try: \n",
        "  del Xq1\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "82YuANMkqdCO"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for i in range(0,q1test_X.shape[0],100000):\n",
        "  if count%2:\n",
        "    print(count)\n",
        "  \n",
        "  subdata = q1test_X.iloc[i:min(i+100000,q1test_X.shape[0]),:]\n",
        "  subdata = pd.merge(left = subdata, right = q1testfeatureset1,left_on = 'lexeme_id',right_on = 'lexeme_id',how=\"left\")\n",
        "  subdata = subdata.drop('lexeme_id',axis=1)\n",
        "  #print('size: ',subdata.size/10**6)\n",
        "\n",
        "  mat = coo_matrix(subdata)\n",
        "  #print('size after compression: ',mat.size/10**6)\n",
        "  if i >1:\n",
        "    print('stacking: ')\n",
        "    Xq1 = vstack([Xq1,mat])\n",
        "    #print('saving: ',\"Xq1_\"+str(count))\n",
        "    #compressed_pickle(path_name+\"Xq1_\"+str(count), mat)\n",
        "  else:\n",
        "    Xq1 = mat\n",
        "    #compressed_pickle(path_name+\"Xq1_\"+str(count), mat)\n",
        "  del subdata, mat\n",
        "  count +=1\n",
        "#print('Saved all, size Xq1: ',Xq1.size/10**6)\n",
        "compressed_pickle(path_name+\"q1test_pipe1\", Xq1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYKR9FKXqIGK",
        "outputId": "2eeb47bb-1985-42e8-bd8b-9a0db8f250c2"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "stacking: \n",
            "stacking: \n",
            "3\n",
            "stacking: \n",
            "stacking: \n",
            "5\n",
            "stacking: \n",
            "stacking: \n",
            "7\n",
            "stacking: \n",
            "stacking: \n",
            "9\n",
            "stacking: \n",
            "stacking: \n",
            "11\n",
            "stacking: \n",
            "stacking: \n",
            "13\n",
            "stacking: \n",
            "stacking: \n",
            "15\n",
            "stacking: \n",
            "stacking: \n",
            "17\n",
            "stacking: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_pickle(path_name+\"q1test_y\",q1test_y)"
      ],
      "metadata": {
        "id": "rfmcfrBcrusF"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'cornflowerblue' size=4>Conclusions and Next Steps</font>\n",
        "\n",
        "Word based features have been joined to the Q1 train and test dataframes, and stored as compressed sparse matrices. \n",
        "\n",
        "scaling will be done before modeling. \n",
        "\n",
        "Next steps:\n",
        "\n",
        "For pipe1:\n",
        "\n",
        "Further split q1train_featureset1 into training and validation sets (90-10) split. \n",
        "\n",
        "1. Model with classical machine learning techniques\n",
        "- downsample or upsample or adjust class weight hyperparameter\n",
        "- logistic regression\n",
        "- decision tree\n",
        "- Naive Bayes classification\n",
        "- downsampled kNN\n",
        "- doensampled SVM\n",
        "\n",
        "2. Ensemble techniques\n",
        "- Random Forest\n",
        "- AdaBoost\n",
        "- XGBoost\n",
        "\n",
        "3. Neural net\n",
        "- Dense neural net. \n",
        "\n",
        "Model performance of pipe1 will be compared with validation set performance average precision and ROC AUC, with a baseline model with no word based features. \n",
        "\n"
      ],
      "metadata": {
        "id": "jNQSCoofsDWT"
      }
    }
  ]
}